{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geethumohan23/Offensive_Tweet_Classification/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgzEm1gDYBUp"
      },
      "source": [
        "Offensive Tweet Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3ZWJlO6JOqY"
      },
      "source": [
        "Installing `transformers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqxPHsUOqVvh",
        "outputId": "36f26090-9f43-4684-f514-93bd268d2bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5XEt6asIi3Q"
      },
      "source": [
        "Importing all require libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKEZRYhIImbg",
        "outputId": "9e3ddb38-bebf-432b-9239-bb32696dfc95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import numpy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd5kSsAPZoE6"
      },
      "source": [
        "##Setting seed with student Id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqP6pp_3ZkVy"
      },
      "outputs": [],
      "source": [
        "id = 2201670\n",
        "\n",
        "#numpy seed\n",
        "np.random.seed(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNzgAO86liL2"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1kvIe1NbDoS",
        "outputId": "538659c1-766f-4067-b385-bb4242bb876a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Z_e-zElvgH"
      },
      "source": [
        "##Setting Drive Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WKnbP3roLTj",
        "outputId": "263b6d35-0c55-4838-8cb7-4989a6766cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List files:  ['models', 'train.csv', 'valid.csv', 'test.csv', 'train_25.csv', 'train_50.csv', 'train_75.csv', 'train_100.csv']\n"
          ]
        }
      ],
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(id))\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdLem9mzWDQo"
      },
      "source": [
        "##Loading File For Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaCVLw5xV-5A",
        "outputId": "c3859a4a-1b09-4963-be29-3d19ad6b4a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12313\n",
            "Each part have 3078 rows\n",
            "         id  \\\n",
            "0     42884   \n",
            "1     92152   \n",
            "2     65475   \n",
            "3     22144   \n",
            "4     81048   \n",
            "...     ...   \n",
            "9230  53662   \n",
            "9231  16140   \n",
            "9232  86772   \n",
            "9233  16321   \n",
            "9234  66166   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                        tweet  \\\n",
            "0     @USER I’m done with you as well. An INTENTIONAL 11th hour attack completely based on hearsay and you want an investigation? What do you think they’ll find other than he said she said. Feinstein admitted she sat on this and here you are ready to lockstep with the swamp.                             \n",
            "1     I now have over 6k followers.  Only 94k to go to get to my goal!  #TrumpTrain #MAGA #KAG                                                                                                                                                                                                                  \n",
            "2     @USER Tom was bought! He is more interested in Tom than his constituents                                                                                                                                                                                                                                  \n",
            "3     @USER @USER Even her brother thinks she is a monster: URL                                                                                                                                                                                                                                                 \n",
            "4     @USER @USER @USER @USER @USER I can understand that and if it’s someone I know I’ll debate with them but I’m not willing to spend that much time trying to change the mind of someone I don’t know I tryed that with people and gun control and it just get stupid because they won’t try to understand   \n",
            "...                                                                                                                                                                                                                                                                                                       ...   \n",
            "9230  @USER Why is Antifa upset w/ “prof.” Blasé-Ford? Antifa is the group that harrasses &amp; threatens people at their homes.                                                                                                                                                                                \n",
            "9231  @USER They are conservatives who want him appointed.                                                                                                                                                                                                                                                      \n",
            "9232  Great tweet! Please retweet! #Patriots #Deplorables @USER @USER @USER @USER @USER #WalkAwayFromDemocrats #BackTheBlue #RapistDemocrats #FraudDemocrats #MAGA URL                                                                                                                                          \n",
            "9233  @USER @USER @USER Yes.......that's why Aus. Conservatives, Bob Katter &amp; Family First have formed government for the last 50 years........because we are inherently conservative\"! #ConservativeComedy\"                                                                                                \n",
            "9234  @USER Dig up Frankie. Save yourself.                                                                                                                                                                                                                                                                      \n",
            "\n",
            "     label  \n",
            "0     NOT   \n",
            "1     NOT   \n",
            "2     NOT   \n",
            "3     OFF   \n",
            "4     OFF   \n",
            "...   ...   \n",
            "9230  OFF   \n",
            "9231  NOT   \n",
            "9232  NOT   \n",
            "9233  NOT   \n",
            "9234  NOT   \n",
            "\n",
            "[12313 rows x 3 columns]\n",
            "Successfully written those files back to the drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-160-5d0609d6e4f9>:40: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        }
      ],
      "source": [
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data\n",
        "train_file\n",
        "df=pd.read_csv(train_file)\n",
        "\n",
        "\n",
        "#Calculating split size, just cross checking\n",
        "n_rows = len(df)\n",
        "print(n_rows)\n",
        "\n",
        "\n",
        "# Split the data into four equal parts\n",
        "part_size = n_rows // 4\n",
        "print(\"Each part have\", part_size, \"rows\")\n",
        "\n",
        "#Spliting the data frame into 4 parts\n",
        "parts = np.array_split(df, 4)\n",
        "part_1=parts[0]\n",
        "part_2=parts[1]\n",
        "part_3=parts[2]\n",
        "part_4=parts[3]\n",
        "\n",
        "\n",
        "#Splitting data into 25%, 50%, 75%, 100%\n",
        "train_25=part_1\n",
        "train_50=pd.concat([train_25,part_2])\n",
        "train_75=pd.concat([train_50,part_3])\n",
        "train_100=pd.concat([train_75,part_3])\n",
        "print(train_100)\n",
        "\n",
        "\n",
        "#Writing back to the drive\n",
        "train_25.to_csv(\"/content/gdrive/MyDrive/CE807/Assignment2/2201670/train_25.csv\", index=False)\n",
        "train_50.to_csv(\"/content/gdrive/MyDrive/CE807/Assignment2/2201670/train_50.csv\", index=False)\n",
        "train_75.to_csv(\"/content/gdrive/MyDrive/CE807/Assignment2/2201670/train_75.csv\", index=False)\n",
        "train_100.to_csv(\"/content/gdrive/MyDrive/CE807/Assignment2/2201670/train_100.csv\", index=False)\n",
        "print(\"Successfully written those files back to the drive\")\n",
        "\n",
        "\n",
        "#Detainling of data\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "train_100= train_100 [['id','tweet', 'label']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fibMnjZWRtp"
      },
      "source": [
        "## Naming Directories !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQiwiE7BWXyQ",
        "outputId": "0da29b81-0cca-44c9-8cfb-60c91f57102d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1 directory:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1\n",
            "Model 1 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25\n",
            "Model 1 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50\n",
            "Model 1 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75\n",
            "Model 1 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100\n",
            "Model 2 directory:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2\n",
            "Model 2 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25\n",
            "Model 2 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50\n",
            "Model 2 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75\n",
            "Model 2 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100\n",
            "Output file name using model 1 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/output_test.csv\n",
            "Output file name using model 1 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/output_test.csv\n",
            "Output file name using model 1 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/output_test.csv\n",
            "Output file name using model 1 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/output_test.csv\n",
            "Output file name using model 2 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/output_test.csv\n",
            "Output file name using model 2 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/output_test.csv\n",
            "Output file name using model 2 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/output_test.csv\n",
            "Output file name using model 2 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Model 1 directory\n",
        "\n",
        "MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1')\n",
        "print('Model 1 directory: ', MODEL_1_DIRECTORY)\n",
        "\n",
        "MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory\n",
        "print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)\n",
        "\n",
        "MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory\n",
        "print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)\n",
        "\n",
        "MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory\n",
        "print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)\n",
        "\n",
        "MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory\n",
        "print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)\n",
        "\n",
        "# Model 2 directory\n",
        "\n",
        "MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 1 directory\n",
        "print('Model 2 directory: ', MODEL_2_DIRECTORY)\n",
        "\n",
        "MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 1 trained using 25% of train data directory\n",
        "print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)\n",
        "\n",
        "MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 1 trained using 50% of train data directory\n",
        "print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)\n",
        "\n",
        "MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 1 trained using 75% of train data directory\n",
        "print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)\n",
        "\n",
        "MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 1 trained using 100% of train data directory\n",
        "print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)\n",
        "\n",
        "#Model 1 output file\n",
        "\n",
        "model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)\n",
        "\n",
        "model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data\n",
        "print('Output file name using model 1 using 50% of train data: ',model_1_50_output_test_file)\n",
        "\n",
        "model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data\n",
        "print('Output file name using model 1 using 75% of train data: ',model_1_75_output_test_file)\n",
        "\n",
        "model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data\n",
        "print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)\n",
        "\n",
        "#Model 2 output file\n",
        "\n",
        "model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 2 using 25% of train data: ',model_2_25_output_test_file)\n",
        "\n",
        "model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data\n",
        "print('Output file name using model 2 using 50% of train data: ',model_2_50_output_test_file)\n",
        "\n",
        "model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data\n",
        "print('Output file name using model 2 using 75% of train data: ',model_2_75_output_test_file)\n",
        "\n",
        "model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data\n",
        "print('Output file name using model 2 using 100% of train data: ',model_2_100_output_test_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf7L71sUwtzl",
        "outputId": "73d56f7a-9b02-4f61-964f-bc75b60c745a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 3 directory:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3\n",
            "Model 3 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/25\n",
            "Model 3 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/50\n",
            "Model 3 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/75\n",
            "Model 3 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/100\n",
            "Output file name using model 3 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/25/output_test.csv\n",
            "Output file name using model 3 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/50/output_test.csv\n",
            "Output file name using model 3 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/75/output_test.csv\n",
            "Output file name using model 3 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/100/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Model 3 directory\n",
        "\n",
        "MODEL_3_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '3') # Model 3 directory\n",
        "print('Model 3 directory: ', MODEL_3_DIRECTORY)\n",
        "\n",
        "MODEL_3_25_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'25') # Model 3 trained using 25% of train data directory\n",
        "print('Model 3 directory with 25% data: ', MODEL_3_25_DIRECTORY)\n",
        "\n",
        "MODEL_3_50_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'50') # Model 3 trained using 50% of train data directory\n",
        "print('Model 3 directory with 50% data: ', MODEL_3_50_DIRECTORY)\n",
        "\n",
        "MODEL_3_75_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'75') # Model 3 trained using 25% of train data directory\n",
        "print('Model 3 directory with 75% data: ', MODEL_3_75_DIRECTORY)\n",
        "\n",
        "MODEL_3_100_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'100') # Model 3 trained using 25% of train data directory\n",
        "print('Model 3 directory with 25% data: ', MODEL_3_100_DIRECTORY)\n",
        "\n",
        "#Output Directory\n",
        "model_3_25_output_test_file = os.path.join(MODEL_3_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 3 using 25% of train data: ',model_3_25_output_test_file)\n",
        "\n",
        "model_3_50_output_test_file = os.path.join(MODEL_3_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 3 using 50% of train data: ',model_3_50_output_test_file)\n",
        "\n",
        "model_3_75_output_test_file = os.path.join(MODEL_3_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 3 using 75% of train data: ',model_3_75_output_test_file)\n",
        "\n",
        "model_3_100_output_test_file = os.path.join(MODEL_3_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data\n",
        "print('Output file name using model 3 using 100% of train data: ',model_3_100_output_test_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAFgClNzsaLd"
      },
      "source": [
        "##Writing Back the splitted files to the target folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc5LA3O42f5C",
        "outputId": "3620f0ee-810a-4c9d-b7c2-c809932b24f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 25% file:  gdrive/MyDrive/./CE807/Assignment2/2201670/train_25.csv\n",
            "Train 50% file:  gdrive/MyDrive/./CE807/Assignment2/2201670/train_50.csv\n",
            "Train 75% file:  gdrive/MyDrive/./CE807/Assignment2/2201670/train_75.csv\n",
            "Train 100% file:  gdrive/MyDrive/./CE807/Assignment2/2201670/train_100.csv\n",
            "Validation file:  gdrive/MyDrive/./CE807/Assignment2/2201670/valid.csv\n",
            "Test file:  gdrive/MyDrive/./CE807/Assignment2/2201670/test.csv\n"
          ]
        }
      ],
      "source": [
        "train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv') #Let's assume that you have train 25% file is saved in train_25.csv. Note that this is a dummy file. You have to create your own file.\n",
        "train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')\n",
        "train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')\n",
        "train_100_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv')\n",
        "\n",
        "print('Train 25% file: ', train_25_file)\n",
        "print('Train 50% file: ', train_50_file)\n",
        "print('Train 75% file: ', train_75_file)\n",
        "print('Train 100% file: ', train_100_file)\n",
        "\n",
        "val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')\n",
        "print('Validation file: ', val_file)\n",
        "\n",
        "test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n",
        "print('Test file: ', test_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWvXDghtafa"
      },
      "source": [
        "### Ratio of NOT and OFF labels in the training, test and validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "siWoP_EjrRkf",
        "outputId": "975c6c10-b7aa-40d3-f57e-68c2a34ecb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The whole data set contain: id       12313\n",
            "tweet    12313\n",
            "label    12313\n",
            "dtype: int64 rows\n",
            "Label count in 100 % training data \n",
            " NOT: 66.6937383253472 \n",
            " OFF: 33.3062616746528\n",
            "The whole data set contain: id       9235\n",
            "tweet    9235\n",
            "label    9235\n",
            "dtype: int64 rows\n",
            "Label count in 75 % training data \n",
            " NOT: 66.88684353004872 \n",
            " OFF: 33.11315646995127\n",
            "The whole data set contain: id       6157\n",
            "tweet    6157\n",
            "label    6157\n",
            "dtype: int64 rows\n",
            "Label count in 50 % training data \n",
            " NOT: 67.27302257592983 \n",
            " OFF: 32.726977424070164\n",
            "The whole data set contain: id       3079\n",
            "tweet    3079\n",
            "label    3079\n",
            "dtype: int64 rows\n",
            "Label count in 25 % training data \n",
            " NOT: 66.09288730107178 \n",
            " OFF: 33.90711269892822\n"
          ]
        }
      ],
      "source": [
        "print(\"The whole data set contain:\", train_100.count(), \"rows\")\n",
        "print(f\"Label count in 100 % training data \\n NOT:\",((train_100['label']=='NOT').sum()/train_100['label'].count())*100, \"\\n OFF:\",((train_100['label']=='OFF').sum()/train_100['label'].count())*100)\n",
        "print(\"The whole data set contain:\", train_75.count(), \"rows\")\n",
        "print(f\"Label count in 75 % training data \\n NOT:\",((train_75['label']=='NOT').sum()/train_75['label'].count())*100, \"\\n OFF:\",((train_75['label']=='OFF').sum()/train_75['label'].count())*100)\n",
        "print(\"The whole data set contain:\", train_50.count(), \"rows\")\n",
        "print(f\"Label count in 50 % training data \\n NOT:\",((train_50['label']=='NOT').sum()/train_50['label'].count())*100, \"\\n OFF:\",((train_50['label']=='OFF').sum()/train_50['label'].count())*100)\n",
        "print(\"The whole data set contain:\", train_25.count(), \"rows\")\n",
        "print(f\"Label count in 25 % training data \\n NOT:\",((train_25['label']=='NOT').sum()/train_25['label'].count())*100, \"\\n OFF:\",((train_25['label']=='OFF').sum()/train_25['label'].count())*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6OGwZ0yZq-3",
        "outputId": "87dc93d3-dd0a-44e7-c876-409b390af132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The whole data set contain: id       927\n",
            "tweet    927\n",
            "label    927\n",
            "dtype: int64 rows\n",
            "Label count in validation  file: \n",
            " NOT: 66.77454153182309 \n",
            " OFF: 33.22545846817692\n"
          ]
        }
      ],
      "source": [
        "val_fil = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv') # This is 100% of data\n",
        "val_fil\n",
        "val_fil=pd.read_csv(val_fil)\n",
        "print(\"The whole data set contain:\", val_fil.count(), \"rows\")\n",
        "print(f\"Label count in validation  file: \\n NOT:\",((val_fil['label']=='NOT').sum()/val_fil['label'].count())*100, \"\\n OFF:\",((val_fil['label']=='OFF').sum()/val_fil['label'].count())*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZWpZacsa3C8",
        "outputId": "89e3d2f3-ddcc-4219-ac3b-320ceb810fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The whole data set contain: id       860\n",
            "tweet    860\n",
            "label    860\n",
            "dtype: int64 rows\n",
            "Label count in test  file: \n",
            " NOT: 72.09302325581395 \n",
            " OFF: 27.906976744186046\n"
          ]
        }
      ],
      "source": [
        "test_fil = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv') # This is 100% of data\n",
        "test_fil\n",
        "test_fil=pd.read_csv(test_fil)\n",
        "print(\"The whole data set contain:\", test_fil.count(), \"rows\")\n",
        "print(f\"Label count in test  file: \\n NOT:\",((test_fil['label']=='NOT').sum()/test_fil['label'].count())*100, \"\\n OFF:\",((test_fil['label']=='OFF').sum()/test_fil['label'].count())*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-OjJ4REbhcj"
      },
      "source": [
        "We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLuUu5BWcTid"
      },
      "outputs": [],
      "source": [
        "def compute_performance(y_true, y_pred, split='test'):\n",
        "    \"\"\"\n",
        "    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).\n",
        "    This also display Confusion Matrix with proper X & Y axis labels.\n",
        "    Also, returns F1 score\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array or list\n",
        "        y_pred: numpy array or list\n",
        "        split: str\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "    \"\"\"\n",
        "\n",
        "    print('Computing different preformance metrics on', split, ' set of Dataset')\n",
        "    f1score=f1_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print('F1 Score(macro): ', f1score)\n",
        "    print('Accuracy: ', acc)\n",
        "\n",
        "    return f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ywe8jGSKhL"
      },
      "source": [
        "# Method 1 Start\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sA3OWlVbnoY"
      },
      "source": [
        "## Training code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJyIgPsAapum"
      },
      "source": [
        "Function to vectorizing the data, see the difference training and other cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkzfmz1VapS-"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset1(data, vectorizer=None, split='test'):\n",
        "  # Removing numbers from the tweet\n",
        "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
        "\n",
        "  # Changing upper case to lower case\n",
        "  data['tweet']= data['tweet'].apply(lambda x: x.lower())\n",
        "\n",
        "  # Removing stop words\n",
        "  stop_words = stopwords.words('english')\n",
        "  data['tweet'] = data['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "  # convert the tweet column to string type\n",
        "  data['tweet'] = data['tweet'].astype(str)\n",
        "\n",
        "  # Removing pountuations\n",
        "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n",
        "  #Stemming words to its row form\n",
        "  stemmer= PorterStemmer()\n",
        "  stemmer.stem(\"tweet\")\n",
        "\n",
        "\n",
        "  if split == 'train':\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # fit and transform the documents\n",
        "    values = vectorizer.fit_transform(data['tweet'].values)\n",
        "    return values, vectorizer\n",
        "  else:\n",
        "    values = vectorizer.transform(data['tweet'].values)\n",
        "    return values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtXYf8trgBM-"
      },
      "source": [
        "SVM training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqv3isARdDek"
      },
      "outputs": [],
      "source": [
        "def train_model1(text_vector,label):\n",
        "  print(\"In classifier\")\n",
        "  # define the SVM pipeline with TF-IDF vectorizer and SVM classifier\n",
        "  svm_classifier = SVC(kernel=\"linear\")\n",
        "  svm_classifier.fit(text_vector, label)\n",
        "  return svm_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aHXeTabgDGN"
      },
      "source": [
        "Save model and count vectorizer as pickel in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX5Q4BYLgAko"
      },
      "outputs": [],
      "source": [
        "def save_model1(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav')\n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pk6_0AkghWX"
      },
      "source": [
        "Load model and count vectorizer  as pickel from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy0WpA4jgmoa"
      },
      "outputs": [],
      "source": [
        "def load_model1(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Method 1"
      ],
      "metadata": {
        "id": "yAn_dW00DN_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ux7GBKXbqq6"
      },
      "outputs": [],
      "source": [
        "def train_method1(train_file, val_file, model_dir):\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "\n",
        "    train_values, count_vectorizer = prepare_dataset1(train_df, split='train')\n",
        "    val_values= prepare_dataset1(val_df,count_vectorizer)\n",
        "\n",
        "    model = train_model1(train_values,train_label)\n",
        "\n",
        "    model_file, vectorizer_file = save_model1(model, count_vectorizer, model_dir)\n",
        "\n",
        "    train_pred_label = model.predict(train_values)\n",
        "    val_pred_label = model.predict(val_values)\n",
        "\n",
        "    # print('Train Split')\n",
        "    train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRiNC3KmWq8C"
      },
      "source": [
        "Let's train using 100% of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csv_SZ5LcRWY",
        "outputId": "ea34f9e9-a945-43b1-aa2a-81eb912a47e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 100% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/vectorizer.sav\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method1(train_100_file, val_file, MODEL_1_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwyiYc7aXK5i"
      },
      "source": [
        "Training Using 25% of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoEvlMP7Z5LC",
        "outputId": "ff8763e6-d261-4a95-ea36-1a85dfbddc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 25% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9208846248112394\n",
            "Accuracy:  0.9324455992205262\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6616788321167884\n",
            "Accuracy:  0.7389428263214671\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 25% of data')\n",
        "model_25_file, vectorizer_25_file = train_method1(train_25_file, val_file, MODEL_1_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training 50% data"
      ],
      "metadata": {
        "id": "B14GHhB4DUIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2N6J8TBDUNR",
        "outputId": "c6b4f1b7-5a42-49fa-f8b3-ad623b2c4b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 50% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.8824687009783188\n",
            "Accuracy:  0.9040116940068215\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6865431803219821\n",
            "Accuracy:  0.756202804746494\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 50% of data')\n",
        "model_50_file, vectorizer_50_file = train_method1(train_50_file, val_file, MODEL_1_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training 75% data"
      ],
      "metadata": {
        "id": "iX-rfTUiDXq3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2HVQ_RFEISz",
        "outputId": "1c82e107-4d09-4fc8-df53-6a395e8a1cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 75% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.8675552925422907\n",
            "Accuracy:  0.8909583107742285\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6913205167495697\n",
            "Accuracy:  0.756202804746494\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 75% of data')\n",
        "model_75_file, vectorizer_75_file = train_method1(train_75_file, val_file, MODEL_1_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyJ_xv12Uy9c"
      },
      "source": [
        "# Testing code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43T3JqK5a484"
      },
      "outputs": [],
      "source": [
        "def test_method1(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "\n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model1(model_file, vectorizer_file)\n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved\n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "\n",
        "    # return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd-JunUfXdVK"
      },
      "source": [
        "Let's test using model trained on 100% data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0tXyGTyiDt2",
        "outputId": "8df456b4-e958-45cc-e52e-f0231a311f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/vectorizer.sav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7374045801526719\n",
            "Accuracy:  0.8093023255813954\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/100/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method1(test_file, model_100_file, vectorizer_100_file, MODEL_1_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatXpm4HXm8y"
      },
      "source": [
        "Let's test using model trained on 25% data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlAQo-8DXo3p",
        "outputId": "9fa66dba-ddad-4c53-f031-d647e7daf177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 25% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/vectorizer.sav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7004812563323202\n",
            "Accuracy:  0.7953488372093023\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/25/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method1(test_file, model_25_file, vectorizer_25_file, MODEL_1_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing 50% data"
      ],
      "metadata": {
        "id": "8TlUBs5qDizG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT9_pK_eD4oz",
        "outputId": "6a3c92c8-ffd7-4da4-9609-4bb0ddb3359c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 50% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/vectorizer.sav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7095771984330677\n",
            "Accuracy:  0.8023255813953488\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/50/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method1(test_file, model_50_file, vectorizer_50_file, MODEL_1_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIMWtk6qD429",
        "outputId": "69cd2549-0f80-4364-f578-40c9d1ff6f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 75% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/vectorizer.sav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-efd88cef947f>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-13-efd88cef947f>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7263943440691281\n",
            "Accuracy:  0.8116279069767441\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/1/75/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method1(test_file, model_75_file, vectorizer_75_file, MODEL_1_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue3xIDFGSXNH"
      },
      "source": [
        "## Method 1 End\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmaJfJkVwSDW"
      },
      "source": [
        "# Method 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxjuZno2Lgct",
        "outputId": "474f6fd9-83a2-4745-cad7-43312615762a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9xiSqBEUufU"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "def train_model2(text_vector,label):\n",
        "  print(\"In classifier\")\n",
        "  # define the SVM pipeline with TF-IDF vectorizer and SVM classifier\n",
        "  NB_classifier = MultinomialNB()\n",
        "  NB_classifier.fit(text_vector, label)\n",
        "  return NB_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQTmVWa5wdD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-6iAWX7_uLW"
      },
      "outputs": [],
      "source": [
        "def save_model2(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav')\n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTS3h15w_x8o"
      },
      "outputs": [],
      "source": [
        "def load_model2(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x9N7vrX_1d9"
      },
      "outputs": [],
      "source": [
        "def train_method2(train_file, val_file, model_dir):\n",
        "  train_df = pd.read_csv(train_file)\n",
        "  val_df = pd.read_csv(val_file)\n",
        "\n",
        "  train_label = train_df['label']\n",
        "  val_label = val_df['label']\n",
        "  train_values, vectorizer = prepare_dataset1(train_df, split='train')\n",
        "  val_values= prepare_dataset1(val_df,vectorizer)\n",
        "\n",
        "  model = train_model2(train_values,train_label)\n",
        "\n",
        "  model_file, vectorizer_file = save_model2(model, vectorizer,model_dir )\n",
        "  print(\"Completed saving model and vectorizer\")\n",
        "  train_pred_label = model.predict(train_values)\n",
        "  print(\"passed predicion\")\n",
        "  val_pred_label = model.predict(val_values)\n",
        "\n",
        "  # print('Train Split')\n",
        "  train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "  val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "  return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHLRUwxEju4",
        "outputId": "c1a03db5-f5d1-4746-b3c0-a336823225c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/vectorizer.sav\n",
            "Completed saving model and vectorizer\n",
            "passed predicion\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.7667372996970758\n",
            "Accuracy:  0.8259563063428896\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.5673023572866632\n",
            "Accuracy:  0.7141316073354909\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method2(train_100_file, val_file, MODEL_2_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt7dtT_gEjxe",
        "outputId": "e0ae3cad-4805-418a-9e9d-a1a0f928dc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 25% of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/vectorizer.sav\n",
            "Completed saving model and vectorizer\n",
            "passed predicion\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.7463332140466157\n",
            "Accuracy:  0.8119519324455993\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.520948237792331\n",
            "Accuracy:  0.703344120819849\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 25% of data')\n",
        "model_25_file, vectorizer_25_file = train_method2(train_25_file, val_file, MODEL_2_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK-F53I2Ej04",
        "outputId": "86ac5575-ed2c-49b4-fc5b-476c32a1c078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 50% of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/vectorizer.sav\n",
            "Completed saving model and vectorizer\n",
            "passed predicion\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.6945672268947616\n",
            "Accuracy:  0.7886957934058795\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.5373367497073137\n",
            "Accuracy:  0.7098166127292341\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 50% of data')\n",
        "model_50_file, vectorizer_50_file = train_method2(train_50_file, val_file, MODEL_2_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79tJiK7xEj3V",
        "outputId": "faf6dd4b-a635-4965-d73c-9dc2e81420f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 75% of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/vectorizer.sav\n",
            "Completed saving model and vectorizer\n",
            "passed predicion\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.687731156403607\n",
            "Accuracy:  0.7828911748781808\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.5524752764936509\n",
            "Accuracy:  0.7152103559870551\n"
          ]
        }
      ],
      "source": [
        "print('Train using of 75% of data')\n",
        "model_75_file, vectorizer_75_file = train_method2(train_75_file, val_file, MODEL_2_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cv7qUIsE7cp"
      },
      "source": [
        "Testing Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGelxJkBExDR"
      },
      "outputs": [],
      "source": [
        "def test_method2(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "\n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model2(model_file, vectorizer_file)\n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved\n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "\n",
        "    # return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX_2QUiJy3hU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSl1kqY7ExcG",
        "outputId": "f43efb2d-16ce-4fa2-9ef5-d1219cd18eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/vectorizer.sav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5737004877175929\n",
            "Accuracy:  0.7616279069767442\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method2(test_file, model_100_file, vectorizer_100_file, MODEL_2_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKaLmKrXExsv",
        "outputId": "d0cea8fd-0191-4657-a606-919d9b7dce6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 25% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/vectorizer.sav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5622505661824915\n",
            "Accuracy:  0.7593023255813953\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/output_test.csv\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method2(test_file, model_25_file, vectorizer_25_file, MODEL_2_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X04DQo9YEx6v",
        "outputId": "a3a15740-3678-4d4d-a0ce-88fe991f42e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 50% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5673660225490487\n",
            "Accuracy:  0.7627906976744186\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/output_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method2(test_file, model_50_file, vectorizer_50_file, MODEL_2_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezEKMCbtG4Tp",
        "outputId": "d435bcb1-ba39-4903-de43-79ec85d74041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 75% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5529289447043695\n",
            "Accuracy:  0.7569767441860465\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/output_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method2(test_file, model_75_file, vectorizer_75_file, MODEL_2_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyBv1MyoRxcW",
        "outputId": "bd867578-96ed-4caf-ed7c-60dbcfba42a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yMswIeAwYIf"
      },
      "source": [
        "##MLP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaktzwRXkQez"
      },
      "outputs": [],
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "def train_model3(text_vector,label):\n",
        "  print(\"In classifier\")\n",
        "  max_sequence_length = 100\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation='relu', input_shape=(max_sequence_length,)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  # Convert the sparse matrix to a dense matrix and pad the text sequences to have the same length\n",
        "  dense_sequences = text_vector.toarray()\n",
        "  padded_sequences = pad_sequences(dense_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "  # Convert labels to one-hot encoding\n",
        "  one_hot_labels = to_categorical(label, num_classes=2)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(padded_sequences, one_hot_labels, epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "  # create a sparse tensor with unsorted indices\n",
        "  values = [1, 2, 3]\n",
        "  indices = tf.constant([[0, 1], [1, 0], [0, 0]], dtype=tf.int64)\n",
        "  dense_shape = [2, 2]\n",
        "  #sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
        "  sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=[2, 4])\n",
        "\n",
        "  # Create a correctly ordered copy of the sparse tensor using tf.sparse.reorder\n",
        "  sorted_sparse_tensor = tf.sparse.reorder(sparse_tensor)\n",
        "  # create a copy of the sparse tensor with sorted indices\n",
        "  sorted_indices = tf.sparse.reorder(sparse_tensor).indices\n",
        "  sorted_sparse_tensor = tf.sparse.SparseTensor(indices=sorted_indices, values=values, dense_shape=dense_shape)\n",
        "\n",
        "  # use the sorted sparse tensor in your code\n",
        "  print(\"Passed CLassifier\")\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io680yYbs_k0"
      },
      "outputs": [],
      "source": [
        "def save_model3(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav')\n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EA5Ov4ItE-q"
      },
      "outputs": [],
      "source": [
        "def load_model3(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFH09WBLtI6Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "def train_method3(train_file, val_file, model_dir):\n",
        "  train_df = pd.read_csv(train_file)\n",
        "  val_df = pd.read_csv(val_file)\n",
        "\n",
        "  #Label Encoding\n",
        "  le = LabelEncoder()\n",
        "  train_df['new_label'] = le.fit_transform(train_df['label'])\n",
        "  val_df['new_label'] = le.fit_transform(val_df['label'])\n",
        "\n",
        "  train_label = train_df['new_label']\n",
        "  val_label = val_df['new_label']\n",
        "\n",
        "  #train_label = train_df['label']\n",
        "  #val_label = val_df['label']\n",
        "  train_values, vectorizer = prepare_dataset1(train_df, split='train')\n",
        "  val_values= prepare_dataset1(val_df,vectorizer)\n",
        "\n",
        "  model = train_model3(train_values,train_label)\n",
        "\n",
        "  model_file, vectorizer_file = save_model3(model, vectorizer, model_dir)\n",
        "  print(\"Completed saving model and vectorizer\")\n",
        "  train_pred_label = model.predict(train_values)\n",
        "  print(\"passed predicion\")\n",
        "  val_pred_label = model.predict(val_values)\n",
        "\n",
        "  # print('Train Split')\n",
        "  train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "  val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "  return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training 25% data"
      ],
      "metadata": {
        "id": "2CdirpJlDvMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "kPQHvtVktM75",
        "outputId": "9ad2f140-e534-4d8e-83e5-3369aae2174d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 25% of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classifier\n",
            "Epoch 1/10\n",
            "97/97 [==============================] - 6s 10ms/step - loss: 0.6809 - accuracy: 0.6609\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6621 - accuracy: 0.6609\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6520 - accuracy: 0.6609\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 1s 7ms/step - loss: 0.6463 - accuracy: 0.6609\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6433 - accuracy: 0.6609\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6609\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6411 - accuracy: 0.6609\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6407 - accuracy: 0.6609\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.6609\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 1s 8ms/step - loss: 0.6405 - accuracy: 0.6609\n",
            "Passed CLassifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/25/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/25/vectorizer.sav\n",
            "Completed saving model and vectorizer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-201-320160e60896>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 25% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_25_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_3_25_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-200-c92738438ea1>\u001b[0m in \u001b[0;36mtrain_method3\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed saving model and vectorizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passed predicion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,4495] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
          ]
        }
      ],
      "source": [
        "print('Train using of 25% of data')\n",
        "model_25_file, vectorizer_25_file = train_method3(train_25_file, val_file, MODEL_3_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Training 50% data"
      ],
      "metadata": {
        "id": "PVMgT7BkDy6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "olahBL1VSgR1",
        "outputId": "a315c56c-0a14-45ff-db0f-4fcd2377f4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 50% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Epoch 1/10\n",
            "193/193 [==============================] - 2s 3ms/step - loss: 0.6703 - accuracy: 0.6727\n",
            "Epoch 2/10\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6727\n",
            "Epoch 3/10\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6727\n",
            "Epoch 4/10\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6727\n",
            "Epoch 5/10\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6727\n",
            "Epoch 6/10\n",
            "193/193 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6727\n",
            "Epoch 7/10\n",
            "193/193 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6727\n",
            "Epoch 8/10\n",
            "193/193 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.6727\n",
            "Epoch 9/10\n",
            "193/193 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.6727\n",
            "Epoch 10/10\n",
            "193/193 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6727\n",
            "Passed CLassifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/50/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/50/vectorizer.sav\n",
            "Completed saving model and vectorizer\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-0c9524eb04b0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 50% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_50_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_50_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_50_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_3_50_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-140-c92738438ea1>\u001b[0m in \u001b[0;36mtrain_method3\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed saving model and vectorizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passed predicion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,6715] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
          ]
        }
      ],
      "source": [
        "print('Train using of 50% of data')\n",
        "model_50_file, vectorizer_50_file = train_method3(train_50_file, val_file, MODEL_3_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training 75% data"
      ],
      "metadata": {
        "id": "C0dKdiZ4D0xv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "f7EbtBoTSuZ-",
        "outputId": "18e64e10-71a0-4eaf-9b0e-bcbdae6817db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 75% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Epoch 1/10\n",
            "289/289 [==============================] - 3s 4ms/step - loss: 0.6619 - accuracy: 0.6689\n",
            "Epoch 2/10\n",
            "289/289 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6689\n",
            "Epoch 3/10\n",
            "289/289 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6689\n",
            "Epoch 4/10\n",
            "289/289 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6689\n",
            "Epoch 5/10\n",
            "289/289 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6689\n",
            "Epoch 6/10\n",
            "289/289 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6689\n",
            "Epoch 7/10\n",
            "289/289 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6689\n",
            "Epoch 8/10\n",
            "289/289 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6689\n",
            "Epoch 9/10\n",
            "289/289 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6689\n",
            "Epoch 10/10\n",
            "289/289 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6689\n",
            "Passed CLassifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/75/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/75/vectorizer.sav\n",
            "Completed saving model and vectorizer\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-9aa86e83bcfc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 75% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_75_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_75_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_75_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_3_75_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-140-c92738438ea1>\u001b[0m in \u001b[0;36mtrain_method3\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed saving model and vectorizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passed predicion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,8533] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
          ]
        }
      ],
      "source": [
        "print('Train using of 75% of data')\n",
        "model_75_file, vectorizer_75_file = train_method3(train_75_file, val_file, MODEL_3_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training 100% data"
      ],
      "metadata": {
        "id": "dD6L8rkzD3zB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "pks0I5goSu27",
        "outputId": "7550f324-aebd-4b7c-f622-9b6f5c1b4641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train using of 100% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In classifier\n",
            "Epoch 1/10\n",
            "385/385 [==============================] - 2s 3ms/step - loss: 0.6570 - accuracy: 0.6669\n",
            "Epoch 2/10\n",
            "385/385 [==============================] - 1s 3ms/step - loss: 0.6375 - accuracy: 0.6669\n",
            "Epoch 3/10\n",
            "385/385 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 4/10\n",
            "385/385 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 5/10\n",
            "385/385 [==============================] - 2s 5ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 6/10\n",
            "385/385 [==============================] - 2s 5ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 7/10\n",
            "385/385 [==============================] - 2s 5ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 8/10\n",
            "385/385 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 9/10\n",
            "385/385 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Epoch 10/10\n",
            "385/385 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6669\n",
            "Passed CLassifier\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/100/vectorizer.sav\n",
            "Completed saving model and vectorizer\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-0ae7df1aa5e3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 100% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_100_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_100_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_100_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_3_100_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-140-c92738438ea1>\u001b[0m in \u001b[0;36mtrain_method3\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed saving model and vectorizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passed predicion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,8533] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
          ]
        }
      ],
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method3(train_100_file, val_file, MODEL_3_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing"
      ],
      "metadata": {
        "id": "BY8J2dkJD6YL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OB-o9HKy5H8"
      },
      "outputs": [],
      "source": [
        "def test_method3(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "\n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model3(model_file, vectorizer_file)\n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved\n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "\n",
        "    # return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0_rO2MlzAeX",
        "outputId": "963ccab1-cc48-4e4c-f8e9-7249fa5d3173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 25% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/25/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5622505661824915\n",
            "Accuracy:  0.7593023255813953\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/25/output_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-168-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-168-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method3(test_file, model_25_file, vectorizer_25_file, MODEL_3_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKBHpHDVTU-b",
        "outputId": "228f9247-07bb-447c-b9cf-ab442dc5f081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 50% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/50/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5673660225490487\n",
            "Accuracy:  0.7627906976744186\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/50/output_test.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method3(test_file, model_50_file, vectorizer_50_file, MODEL_3_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m79rZVL0TaFb",
        "outputId": "3511ebd6-9f6e-4a52-d819-6389f3a533c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 75% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/75/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5529289447043695\n",
            "Accuracy:  0.7569767441860465\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/75/output_test.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method3(test_file, model_75_file, vectorizer_75_file, MODEL_3_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_mY6inMTeXr",
        "outputId": "5c87c05b-e028-4dfc-81d8-59bf2225e79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201670/models/2/100/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.5737004877175929\n",
            "Accuracy:  0.7616279069767442\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201670/models/3/100/output_test.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-a7a03c3aebe4>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
            "<ipython-input-108-a7a03c3aebe4>:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data[\"tweet\"] = data['tweet'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method3(test_file, model_100_file, vectorizer_100_file, MODEL_3_100_DIRECTORY)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}